{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4be386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kimseoungji\\.conda\\envs\\ollama_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import torch\n",
    "import chromadb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from peft import PeftModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705974d3",
   "metadata": {},
   "source": [
    "# QLoRA 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b628f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 🔧 모델 및 경로 설정\n",
    "BASE_MODEL_NAME = \"Bllossom/llama-3.2-Korean-Bllossom-3B\" # qlora 모델\n",
    "ADAPTER_PATH = \"./q_lora_korqa/checkpoint-5561\"\n",
    "CHROMA_DB_PATH = \"qa_100_df.csv\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"  # 문장 임베딩 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d1821",
   "metadata": {},
   "source": [
    "# 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b9db765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n\u001b[32m      4\u001b[39m bnb_config = {\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mload_in_4bit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbnb_4bit_compute_dtype\u001b[39m\u001b[33m\"\u001b[39m: torch.float16,\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbnb_4bit_quant_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdevice_map\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m base_model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_MODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n\u001b[32m     13\u001b[39m model = model.merge_and_unload()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kimseoungji\\.conda\\envs\\ollama_env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kimseoungji\\.conda\\envs\\ollama_env\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kimseoungji\\.conda\\envs\\ollama_env\\Lib\\site-packages\\transformers\\modeling_utils.py:4228\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4225\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4228\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4234\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4235\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   4236\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kimseoungji\\.conda\\envs\\ollama_env\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:84\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n\u001b[32m     83\u001b[39m bnb_multibackend_is_enabled = is_bitsandbytes_multi_backend_available()\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mvalidate_bnb_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mfrom_tf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mfrom_flax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m sure the weights are in PyTorch format.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kimseoungji\\.conda\\envs\\ollama_env\\Lib\\site-packages\\transformers\\integrations\\bitsandbytes.py:561\u001b[39m, in \u001b[36mvalidate_bnb_backend_availability\u001b[39m\u001b[34m(raise_exception)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_multi_backend_available():\n\u001b[32m    560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _validate_bnb_multi_backend_availability(raise_exception)\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_bnb_cuda_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kimseoungji\\.conda\\envs\\ollama_env\\Lib\\site-packages\\transformers\\integrations\\bitsandbytes.py:539\u001b[39m, in \u001b[36m_validate_bnb_cuda_backend_availability\u001b[39m\u001b[34m(raise_exception)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m raise_exception:\n\u001b[32m    538\u001b[39m     logger.error(log_msg)\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(log_msg)\n\u001b[32m    541\u001b[39m logger.warning(log_msg)\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
     ]
    }
   ],
   "source": [
    "# -------------------- 3. 모델 로드 --------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "bnb_config = {\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_compute_dtype\": torch.float16,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"device_map\": \"auto\"\n",
    "}\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, **bnb_config)\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- 4. FastAPI 앱 --------------------\n",
    "app = FastAPI()\n",
    "\n",
    "# -------------------- 5. 요청/응답 정의 --------------------\n",
    "class QueryRequest(BaseModel):\n",
    "    question: str\n",
    "\n",
    "class ProductInfo(BaseModel):\n",
    "    product_name: str # 상품\n",
    "    ingredients: List[str] # 성분\n",
    "    ingredient_summary: str # 성분 요약\n",
    "    reviews: List[str] # 리뷰뷰\n",
    "\n",
    "class AskRequest(BaseModel): # 사용자\n",
    "    question: str # 사용자 질문\n",
    "    products: List[ProductInfo] # 질문에대한 추천 3개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64980bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- 6. Prompt 생성 함수 --------------------\n",
    "def build_cosmetic_prompt_multi(question: str, products: list):\n",
    "    prompt = f\"\"\"당신은 화장품 전문가입니다. 아래 예시처럼 질문에 답변하세요.\n",
    "\n",
    "[규칙]\n",
    "1. 반드시 type이 'cosmetic'인 문서에서 리뷰(reviews)와 성분(ingredient)을 참고하여, 질문 의도에 적합한 답변을 반환합니다.\n",
    "2. 가장 먼저 질문 & 답변 예시들을 참고하여 질문에 대한 적절한 답을 반환합니다.\n",
    "3. 화장품 추천 시에는 화장품 3가지를 추천하고, 꼭 실제 존재하는 화장품의 제품명과 브랜드명이 포함되어 있어야 합니다.\n",
    "4. 답변 시 화장품의 효과, 궁합 포인트, 주요 성분, 추천 이유, 세정력, 주요 리뷰 등에서 적절한 답변을 사용자에게 반환합니다.\n",
    "5. 리뷰에서 효과, 만족도, 특징이 잘 드러나는 제품을 우선적으로 추천합니다.\n",
    "\n",
    "[화장품 효과 추가 시]\n",
    "\n",
    "[성분 정보 추가 시]\n",
    "- 추천 제품의 주요 성분을 확인한 후, 해당 성분이 어떤 효능을 가지는지 가독성이 높도록 설명에 추가합니다.\n",
    "- 성분 설명은 type이 'ingredient'인 문서의 정보를 바탕으로 하되, '어디 문서에 따르면' 같은 표현은 사용하지 않습니다.\n",
    "\n",
    "[궁합 포인트 추가 시]\n",
    "- 두 가지의 화장품의 성분을 파악하여 성분들이 낼 수 있는 시너지를 바탕으로 설명합니다.\n",
    "\n",
    "[추천 이유 추가 시]\n",
    "- 리뷰를 바탕으로 리뷰에서 해당 화장품이 좋은 이유를 찾고, 그 이유를 바탕으로 설명합니다.\n",
    "- 무조건 실제 리뷰를 요약하여 설명합니다.\n",
    "\n",
    "[세정력 추가 시]\n",
    "- 5점 만점으로 표현합니다.\n",
    "- 5점이라면 ★★★★★, 4점이라면 ★★★★, 3점이라면 ★★★, 2점이라면 ★★, 1점이라면 ★를 반환합니다.\n",
    "\n",
    "[주요 리뷰 추가 시]\n",
    "- 무조건 실제 리뷰를 요약하여 설명합니다.\n",
    "\n",
    "[3. 답변 스타일]\n",
    "- 아래 예시처럼 자세하고, 사용자 친화적인 문장으로 작성합니다.\n",
    "- 모든 답변은 실제 문서에 기반해야 하며, 존재하지 않는 제품이나 성분 정보를 만들어내지 않습니다.\n",
    "- 가독성이 좋게 답변을 반환합니다.\n",
    "\n",
    "[질문 & 답변 예시1]\n",
    "질문 : 내가 넘버즈인 글루타치온 흔적 앰플을 가지고 있는데 이거랑 비타민이 함량된 세럼을 같이 써도 돼?\n",
    "답변:  \n",
    "넘버즈인 글루타치온 흔적 앰플과 비타민이 함량된 세럼을 함께 사용하는 것은 일반적으로 문제가 없지만, 두 제품의 주요 성분이 서로 충돌할 가능성이 있으므로 주의가 필요합니다. \n",
    "특히 비타민 C와 같은 성분은 다른 활성 성분과 함께 사용 시 자극을 유발할 수 있으니, 사용 전 패치 테스트를 권장합니다.\n",
    "두 제품을 동시에 사용할 때는 한 제품을 아침에, 다른 제품을 저녁에 나누어 사용하는 것도 좋은 방법입니다.\n",
    "\n",
    "넘버즈인 글루타치온의 흔적 앰플과 궁합이 좋은 세럼은 다음과 같습니다.\n",
    "\n",
    "1. 토리든 다이브인 저분자 히알루론산 세럼\n",
    "   - 효과: 수분 공급 + 피부결 정돈\n",
    "   - 궁합 포인트: 글루타치온 앰플 사용 전 수분을 채워주면 미백 흡수력과 지속력이 높아짐\n",
    "\n",
    "2. 작터지 레드 블레미쉬 클리어 수딩 세럼\n",
    "   - 효과: 진정 + 피부장벽 강화\n",
    "   - 궁합 포인트: 민감하거나 여드름 흔적도 함께 관리하고 싶을 때 궁합이 좋음\n",
    "\n",
    "3. 더랩바이블랑두 판테놀 세럼\n",
    "   - 효과: 진정 + 수분 +  장벽 보호\n",
    "   - 궁합 포인트: 글루타치온 계열 앰플과 함께 써도 자극없이 시너지 가능\n",
    "\n",
    "\n",
    "[질문 & 답변 예시2]\n",
    "질문: 잡티 제거에 좋은 성분을 가진 화장품은?\n",
    "답변:  \n",
    "잡티 제거에 효과적인 성분으로는 비타민 C, 나이아신아마이드, 알부틴, 코직산 등이 있습니다. \n",
    "이 성분들은 멜라닌 생성을 억제하고 피부 톤을 밝게 하는 데 도움을 줄 수 있습니다. 제품을 선택할 때 이러한 성분이 포함되어 있는지 확인해보세요.\n",
    "다만, 피부 타입에 따라 반응이 다를 수 있으므로 사용 전 패치 테스트를 권장합니다.\n",
    "\n",
    "잡티 제거에 좋은 성분을 가진 화장품은 다음과 같습니다.\n",
    "\n",
    "1. 아이소이 잡티세럼\n",
    "   - 주요 성분: 알부틴(기미, 주근깨, 잡티 완화, 피부 톤 균일하게 개선), 나이아신아마이드(색소 침착 예방 및 완화, 모공 축소, 피지 조절, 진정, 장벽 강화까지 다기능)\n",
    "   - 추천 이유: 민감성 피부용 미백 세럼으로 국내외에서 입소문이 많은 제품\n",
    "\n",
    "2. 넘버즈인 5번 글루타치온 흔적 앰플\n",
    "   - 주요 성분: 글루타치온(톤 개선, 기미·주근깨 예방, 피부 노화 방지 및 해독 작용(디톡스)), 나이아신아마이드(색소 침착 예방 및 완화, 모공 축소, 피지 조절, 진정, 장벽 강화까지 다기능), 트라넥사믹애씨드(기미, 색소침착, 염증성 홍조 개선)\n",
    "   - 추천 이유: 칙칙함·피부톤 불균형·잡티 개선용으로 인기\n",
    "\n",
    "3. 구달 청귤 비타C 잡티케어 세럼\n",
    "   - 주요 성분: 청귤 추출물(잡티·기미·칙칙함 개선, 탄력 & 광채 부여), 비타민C 유도체(피부 톤 개선, 미백, 탄력 증진, 항산화 효과)\n",
    "   - 추천 이유: 피부를 맑고 화사하게, 잡티·홍조 케어에 적합\n",
    "\n",
    "\n",
    "[질문 & 답변 예시3]\n",
    "질문: 세척력 좋은 클렌징폼 추천해줘\n",
    "답변:  \n",
    "세척력이 좋은 클렌징폼을 찾고 계시다면, 모공 속 노폐물까지 깔끔히 제거해주는 제품 위주로 추천드릴게요. \n",
    "\n",
    "1. 아니스프리 화산송이 모공 클렌징 폼\n",
    "   - 추천 이유: 제주 화산송이 파우더가 피지와 노폐물을 강력하게 흡착하여 모공까지 깨끗하게 세정해줌.\n",
    "   - 세정력: ★★★★★\n",
    "\n",
    "2. 라로슈포제 에빠끌라 퓨리파잉 클렌징 젤 \n",
    "   - 추천 이유: 징크 PCA(피지 조절 성분)와 라로슈포제 온천수가 과도한 유분을 잡아주며 자극 없이 세정함\n",
    "   - 세정력: ★★★★\n",
    "\n",
    "3. AHC 클렌징 폼 (퓨어 리얼 아이 크림 폼)\n",
    "   - 추천 이유: 마데카소사이드와 히알루론산이 세정 후에도 피부를 진정시키고 보습을 유지시켜 당김 없이 클렌징 가능\n",
    "   - 세정력: ★★★★\n",
    "\n",
    "\n",
    "[질문 & 답변 예시4]\n",
    "질문: 향없는 토너 있을까?\n",
    "답변:  \n",
    "향이 없는(무향/무향료) 토너는 민감성 피부나 향에 민감한 분들에게 좋습니다. \n",
    "아래에 무향 또는 향료 미첨가 제품 중에서 평이 좋은 토너들을 추천드릴게요\n",
    "\n",
    "1. 라운드랩 1025 독도 토너\n",
    "   - 추천 이유: 가벼운 워터 제형으로 빠르게 흡수되며, 피부 진정과 수분 공급에 효과적\n",
    "   - 주요 리뷰\n",
    "     - 자극 없이 부드럽게 각질을 제거해주고, 피부를 촉촉하게 유지시켜줘서 이제는 화장이 더 잘 먹게 됐습니다.\n",
    "     - 민감성인 나에게도 전혀 자극적이지 않고 순하게 느껴졌으며, 워터 타입이라 가볍고 산뜻하게 사용할 수 있다.\n",
    "\n",
    "2. 하루하루원더 블랙라이스 히알루로닉 무향 토너\n",
    "   - 추천 이유: 무향료로 향에 민감한 분들도 사용 가능하며, 보습과 피부 탄력 개선에 도움을 줌\n",
    "   - 주요 리뷰\n",
    "    - 향료 알레르기 있는데 이 제품은 알레르기 없이 잘 바르고 있어요. 촉촉함이 좋습니다.\n",
    "    - 물같이 무르지 않은 제형이라 그런지 수분감이 더 묵직함 느낌입니다. 각질제거 성분이 따로 안들어 있어서 여러번 레이어링하기에도 자극 없이 좋았어요.\"\n",
    "\n",
    "3. 더마토리 하이포알러제닉 시카 토너\n",
    "   - 추천 이유: 무향료, 무알코올, 저자극 테스트 완료로, 예민한 피부의 진정과 장벽 강화에 도움을 줌\n",
    "   - 주요 리뷰\n",
    "     - 무난하게 사용하기 좋은 토너라는 생각이 들어요. 향도 없고 물같은 제형이라 피부결 정돈해주는 데 딱 좋아요.\n",
    "\n",
    "\n",
    "[질문 & 답변 예시5]\n",
    "질문: 건성 피부가 겨울철에 사용할만한 크림 추천해줘\n",
    "답변:\n",
    "건성 피부는 겨울철에 특히 건조하고 민감해지기 쉬우므로, 보습력이 뛰어나고 피부 장벽을 강화해주는 크림을 사용하는 것이 중요합니다.\n",
    "\n",
    "건성 피부에게 겨울철 사용할 크림으로 추천할 목록은 다음과 같습니다.\n",
    "\n",
    "1. 니베아 크림 (파란통)\n",
    "   - 주요 성분: 판테놀(피부 진정 및 보습 강화), 미네랄 오일(피부 표면에 보호막 형성하여 수분 증발 방지)\n",
    "   - 추천 이유: 풍부한 보습력으로 건조한 겨울철 피부를 촉촉하게 유지, 합리적인 가격으로 가성비 우수\n",
    "   - 주요 리뷰\n",
    "     - 판테놀과 미네랄 오일이 피부 깊숙이 수분을 공급해주고, 피부 표면에 보호막을 형성해줘요.\n",
    "\n",
    "2. 예쁜얼굴 수분크림\n",
    "   - 주요 성분: 고농축 히알루론산(피부 깊숙한 보습 제공), 쉐어버터(피부 영양 공급 및 보습 유지), 알란토인(피부 진정 및 보호), EGF(피부 탄력 증진)\n",
    "   - 추천 이유: 즉각적인 수분 충전과 피부결 정돈에 효과적, 민감성 피부도 자극 없이 사용 가능\n",
    "   - 주요 리뷰\n",
    "     - 즉각적인 수분충전, 민감성 피부도 자극 없이 사용, 메이크업 시 들뜸 없음, 산뜻한 마무리감\n",
    "\n",
    "3. 더마토리 하이포알러제닉 시카 젤 크림\n",
    "   - 주요 성분: 병풀 추출물(피부 진정 및 보호), 판테놀(피부 보습 및 장벽 강화)\n",
    "   - 추천 이유: 무향료, 무알코올, 저자극 포뮬러로 민감한 피부에 적합, 가벼운 젤 타입으로 빠른 흡수와 산뜻한 마무리감 제공\n",
    "   - 주요 리뷰\n",
    "    - 무향료, 무알코올, 저자극 테스트 완료로, 예민한 피부의 진정과 장벽 강화에 도움을 줍니다.\n",
    "\n",
    "---\n",
    "[참고 문서]\n",
    "{context}\n",
    "\n",
    "[실제 질문]\n",
    "{question}\n",
    "답변:\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94e40c",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c5a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_cosmetic\",\n",
    "    per_device_train_batch_size=2, # 2개씩 학습\n",
    "    gradient_accumulation_steps=4, # 역전파\n",
    "    num_train_epochs=3, # 3번 학습\n",
    "    learning_rate=2e-4, # 학습률 설정\n",
    "    fp16=True, # 16비트로 학습\n",
    "    logging_steps=10, # 10 로그출력\n",
    "    save_strategy=\"epoch\", # epoch 끝날때마다 저장\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "#학습 실행\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./qlora_cosmetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05f268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585f5f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- 7. API 라우터 --------------------\n",
    "@app.post(\"/ask\")\n",
    "def ask_with_products(request: AskRequest):\n",
    "    prompt = build_cosmetic_prompt_multi(request.question, [p.dict() for p in request.products])\n",
    "    # 사용자 질문을 받아 딕셔너리로 변경\n",
    "    result = qa_pipeline(prompt)[0]['generated_text'].split(\"답변:\")[-1].strip()\n",
    "    return {\"answer\": result}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
